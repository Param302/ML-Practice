{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8d5693-d422-4f7c-ac41-d65f9f3de6fa",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (ANN) _from Scratch_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a325cd-2786-4349-bca1-812b6609ab7b",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7962a7eb-df7e-4401-9a9f-adf5d78a59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc204391-d2d1-4d22-add0-bfd1ed355086",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447b467-e3c9-4ec1-b038-beac73fad9bb",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff19d462-a457-4087-8c47-cb8f37df3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, z: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, z: np.ndarray) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318edbd-5c6b-444b-bc16-d9b323d3c79a",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1c03b1-aec9-49ec-a0f3-c6d40deed9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Activation):\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "\n",
    "    def derivative(self, z: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117b415-4049-4f91-b02c-68fcc203fabf",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60521d9b-8974-4ac3-a95c-c5761608ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __call__(self, z: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def derivative(self, z: np.ndarray) -> np.ndarray:\n",
    "        return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb80b0-0310-4e3d-b9f6-6446c3f0319c",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20d61f54-f184-44c6-8eaf-fdb2e746425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __call__(self, z: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def derivative(self, z: np.ndarray) -> np.ndarray:\n",
    "        sigmoid = self(z)\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126b8fa-2b8a-43e4-8663-86b63769690e",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a091d5-12cd-4bcf-9e02-3ad07fb909ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Activation):\n",
    "    def __call__(self, z: np.ndarray) -> np.ndarray:\n",
    "        exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def derivative(self, z: np.ndarray) -> np.ndarray:\n",
    "        z = z.reshape(-1, 1)\n",
    "        return np.diagflat(z) - np.dot(z, z.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8762b3-2306-4a17-af7f-302f37ef169e",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6056b64e-8362-4a1b-a9a6-ac035dace9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __call__(self, z: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def derivative(self, z: np.ndarray) -> np.ndarray:\n",
    "        tanh = self(z)\n",
    "        return 1 - tanh ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545027c2-3957-4e43-9577-8618215762f4",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a7b0d1-b472-4218-8269-6d46459b5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be466794-dd9b-40fe-8a37-ac136f006443",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd0646a2-9087-412e-a297-985df869a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss):\n",
    "    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return np.mean((y_pred - y_true) ** 2) / 2\n",
    "\n",
    "    def derivative(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return (y_pred - y_true) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81004a-f4d9-44c9-9993-bcf0a36d7604",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2878d0-99d6-4c3c-bee1-0cd3085f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy(Loss):\n",
    "    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)  # avoid log(0)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def derivative(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)  # Prevent div by 0\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred) * y_true.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d03321-2d3b-443d-8501-d885cacec4de",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss (Sparse Categorical)\n",
    "_basically one hot encoded_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d629f922-498a-4422-ba3e-467b9f7993fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoricalCrossEntropy(Loss):\n",
    "    def __call__(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)  # prevent log(0)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "    def derivative(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        return (y_pred - y_true) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa4820-8fc9-4670-a5a3-bad56e8b2872",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0ccaabd-73c6-4668-b61b-4517465d36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dA: np.ndarray, lr: float) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342cc01-455d-4879-93e5-ed38a1d3c353",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07bb82b-1de2-4be8-b547-e4ff50f42680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, in_dim: int, out_dim: int, activation: Activation):\n",
    "        self.weights = np.random.random((in_dim, out_dim)) * np.sqrt(1 / in_dim)\n",
    "        self.bias = np.zeros((1, out_dim))\n",
    "        self.activation = activation\n",
    "        self.x = None\n",
    "        self.z = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.x = x\n",
    "        self.z = self.x @ self.weights + self.bias\n",
    "        return self.activation(self.z)\n",
    "\n",
    "    def backward(self, dA: np.ndarray, lr: np.ndarray) -> np.ndarray:\n",
    "        if isinstance(self.activation, Softmax):\n",
    "            dz = dA\n",
    "        else:\n",
    "            dz = dA * self.activation.derivative(self.z)\n",
    "        dw = self.x.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "\n",
    "        # prevent exploding gradients\n",
    "        np.clip(dw, -1, 1, out=dw)\n",
    "        np.clip(db, -1, 1, out=db)\n",
    "\n",
    "        self.weights -= lr * dw\n",
    "        self.bias -= lr * db\n",
    "        return dz @ self.weights.T\n",
    "\n",
    "    @property\n",
    "    def params_(self):\n",
    "        return (self.weights, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc2ed4-f5db-4150-8787-0ac98d13ec48",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f02dff0d-386e-4224-a7aa-9eadec4afcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convo2D(Layer):\n",
    "    def __init__(self, in_channels: int, num_filters: int, kernel_size: int | tuple, stride: int, padding: str = None):\n",
    "        self.in_channels = in_channels\n",
    "        self.num_filters = num_filters\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.kh, self.kw = self.kernel_size\n",
    "\n",
    "        # setting limit to prevent exploding gradients\n",
    "        limit = np.sqrt(1 / (self.in_channels * self.kh * self.kw))\n",
    "        self.kernels = np.random.uniform(-limit, limit, (num_filters, in_channels, self.kh, self.kw))\n",
    "        self.bias = np.zeros((num_filters, 1))\n",
    "\n",
    "        self.x = None\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.x = x\n",
    "        # N: input images, c_in: input channels, (H, W): Height, Width\n",
    "        N, c_in, H, W = x.shape\n",
    "\n",
    "        H_out = (H + 2 * self.padding - self.kh) // self.stride + 1\n",
    "        W_out = (W + 2 * self.padding - self.kw) // self.stride + 1\n",
    "\n",
    "        pad_x = np.pad(x, (\n",
    "                           (0, 0), (0, 0),\n",
    "                           (self.padding, self.padding),\n",
    "                           (self.padding, self.padding)\n",
    "                       ))\n",
    "\n",
    "        output_x = np.zeros((N, self.num_filters, H_out, W_out))\n",
    "\n",
    "        for image in range(N):\n",
    "            for filter_ in range(self.num_filters):\n",
    "                for h in range(H_out):\n",
    "                    for w in range(W_out):\n",
    "                        h_start = h * self.stride\n",
    "                        w_start = w * self.stride\n",
    "                        h_end = h_start + self.kh\n",
    "                        w_end = w_start + self.kw\n",
    "                        # image window where kernel filter will be applied\n",
    "                        window = pad_x[image, :, h_start:h_end, w_start:w_end]\n",
    "                        # dot product: wx; w = kernel filter;;\n",
    "                        output_x[image, filter_, h, w] = np.sum(window * self.kernels[filter_]) + self.bias[filter_]\n",
    "\n",
    "        return output_x\n",
    "\n",
    "\n",
    "    def backward(self, dA: np.ndarray, lr: int) -> np.ndarray:\n",
    "        N, c_in, H, W = self.x.shape\n",
    "        _, _, H_out, W_out = dA.shape\n",
    "\n",
    "\n",
    "        pad_x = np.pad(self.x, (\n",
    "                           (0, 0), (0, 0),\n",
    "                           (self.padding, self.padding),\n",
    "                           (self.padding, self.padding)\n",
    "                       ))\n",
    "\n",
    "        dx_padded = np.zeros_like(pad_x)\n",
    "        dk = np.zeros_like(self.kernels)\n",
    "        db = np.zeroes_like(self.bias)\n",
    "\n",
    "        for image in range(N):\n",
    "            for filter_ in range(self.num_filters):\n",
    "                for h in range(H_out):\n",
    "                    for w in range(W_out):\n",
    "                        h_start = h * self.stride\n",
    "                        w_start = w * self.stride\n",
    "                        h_end = h_start + self.kh\n",
    "                        w_end = w_start + self.kw\n",
    "\n",
    "                        window = pad_x[image, : h_start:h_end, w_start:w_end]\n",
    "\n",
    "                        dk[filter_] += dA[image, filter_, h, w] * window\n",
    "                        dx_padded[image, :, h_start:h_end, w_start:w_end] += dA[image, filter_, h, w] * self.kernels[filter_]\n",
    "                        db[filter_] += dA[image, filter_, h, w]\n",
    "\n",
    "        np.clip(dk, -1, 1, out=dk)\n",
    "        np.clip(db, -1, 1, out=db)\n",
    "\n",
    "        self.kernels -= lr * dk\n",
    "        self.bias -= lr * db\n",
    "\n",
    "        # remove padding from dx\n",
    "        if self.padding > 0:\n",
    "            dx = dx_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        else:\n",
    "            dx = dx_padded\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "        @property\n",
    "        def params_(self):\n",
    "            return (self.kernels, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a38a5-0cf3-44dd-b3fd-6f0cbcbdb8c9",
   "metadata": {},
   "source": [
    "### Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d45a7-1a6d-4509-a3a6-f7de003df15a",
   "metadata": {},
   "source": [
    "#### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b92d4-da38-418f-8114-78603797b0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acc1bc1c-0726-498a-a970-3fa88b9b589f",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3435bbca-c658-47b3-9c6a-58c3acf9f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: list[Layer], loss: Loss):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss\n",
    "\n",
    "    def add(self, layer: Layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray, lr: float):\n",
    "        dA = self.loss_fn.derivative(y_pred, y_true)\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA, lr)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        val_X: np.ndarray = None,\n",
    "        val_y: np.ndarray = None,\n",
    "        n_iters: int = 100,\n",
    "        lr: float = 0.01,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.history = []\n",
    "        epoch_10pct = 1 if n_iters < 10 else int(n_iters * 0.1)\n",
    "\n",
    "        for epoch in range(1, n_iters + 1):\n",
    "            y_pred = self.forward(X)\n",
    "            train_loss = self.loss_fn(y_pred, y)\n",
    "            self.backward(y_pred, y, lr)\n",
    "\n",
    "            if val_X is not None and val_y is not None:\n",
    "                y_val_pred = self.forward(val_X)\n",
    "                val_loss = self.loss_fn(y_val_pred, val_y)\n",
    "\n",
    "            self.history.append(train_loss if val_X is None else (train_loss, val_loss))\n",
    "\n",
    "            if verbose and epoch % epoch_10pct == 0:\n",
    "                l = len(str(n_iters))\n",
    "                print(\n",
    "                    f\"Epoch {epoch:{len(str(n_iters))}d} | Train Loss: {train_loss:.4f}\" +\n",
    "                    (\"\" if val_X is None else f\" | Val Loss: {val_loss:.4f}\")\n",
    "                     )\n",
    "        return self.history\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(x)\n",
    "\n",
    "    @property\n",
    "    def params_(self) -> np.ndarray:\n",
    "        return [layer.params_ for layer in self.layers]\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"Model Summary:\")\n",
    "        print(\"-\" * 60)\n",
    "        total_params = 0\n",
    "\n",
    "        for i, layer in enumerate(self.layers, start=1):\n",
    "            name = layer.__class__.__name__\n",
    "            w_shape = layer.weights.shape\n",
    "            b_shape = layer.bias.shape\n",
    "            num_params = np.prod(w_shape) + np.prod(b_shape)\n",
    "            total_params += num_params\n",
    "            print(f\"{name} layer {i}: Params: {num_params}\"\n",
    "                  f\" ({w_shape[0]} x {w_shape[1]} + {b_shape[1]})\")\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Total Layers: {i}\")\n",
    "        print(f\"Total Trainable Parameters: {total_params}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath: str) -> \"NeuralNetwork\":\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b7d85-786b-4681-b691-b6e4d84c273e",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f88852f-c575-4975-ba0f-0bbebace077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_BASE_PATH = \"../Models/Custom_CNN/\"\n",
    "os.makedirs(MODEL_BASE_PATH, exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
